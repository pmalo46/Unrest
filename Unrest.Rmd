---
title: "Unrest"
author: "Patrick Maloney"
date: "3/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Intro

 This Project will look at events of civil unrest, and use historical data to forecast the potential duration of such events.

### Importing data

This data comes from the SPEED data set, which was run out of the University of Illinois and compiles unrest events from 1946 to 2005. I have taken a subset of the data that includes events outside the US that lasted longer than one day.

```{r}
library(RCurl)
library(tidyverse)
library(VIM)
library(dplyr)

data_raw <- read.csv("https://raw.githubusercontent.com/pmalo46/Unrest/main/civil-unrest-event-data-QueryResult.csv")

tail(data_raw, 25)

```


### EDA

This data encompasses a number of decades and covers events across the world, so there is likely to be lots of missing data. View the codebook file in this project  repository for the SPEED project's definition of each variable in the dataset.

```{r}
str(data_raw)
summary(data_raw)
```

```{r}
aggr(data_raw)
```

I need to drop the columns that are majority missing values. 

```{r}
cond1 <- sapply(data_raw, function(col) sum(is.na(col)) > 2000)
cond2 <- sapply(data_raw, function(col) sum(col == "") > 2000)
mask <- !(cond1 | cond2)
df <- data_raw[, !cond1, drop=FALSE]
summary(df)
str(df)
```

```{r}
df <- df[-c(5:13, 25:29)] 
str(df)
```

I have now removed columns with a majority of missing values and dropped columns with specific names like organizations perpatraiting events or victim groups. I have also removed columns containing metadata from the publications.

```{r}
aggr(df)
summary(df)
```

```{r}
boxplot(df$day_span)
summary(df$day_span)
qqnorm(df$day_span, pch = 1, frame = FALSE)
qqline(df$day_span, col = "steelblue", lwd = 2)
```

I was planning to use day_span as my response variable for a multivariate linear regression model, but after checking the conditions for regression, I can see that this variable is not normally distributed. A classification model might be a better choice. Now The data types must be fixed. Many categorical variables are listed as integers, where they should be factors, since the numbers correspond to categorical values, not actual integers.

```{r}
df <-  subset(df, select = -c(eventid, aid, year, code_year, code_month, code_day, code_date, gp3, gp4, gp7, gp8, ambig_wgt, from_eid, to_eid, pinpoint, date_typ, ctry_bias, quasi_event, n_killed_p))

df$month <- factor(df$month)
df$know_ini <- factor(df$know_ini)
df$ev_type <- factor(df$ev_type)
df$loc_type <- factor(df$loc_type)
df$weap_grd <- factor(df$weap_grd)
df$e_length <- factor(df$e_length)
df$region <- factor(df$region)
df$weapon <- factor(df$weapon)

str(df)
```

Now that we have dropped unhelpful columns, let's slice the data to only explore destabalizing events and ignore quasi-events.

```{r}
df1 <- filter(df, event==TRUE)
head(df1, 25)

```

The e_length variable seems to have some discrepancy with the day_span value. In many cases the values don't match up. Therefore, I'm going to create a new variable for event duration that will put the data into buckets based on day_span

```{r}
df1 <- df1 %>% mutate(duration = case_when(day_span <= 7 ~ "1",
                            day_span > 7 & day_span <= 30 ~ "2", 
                           day_span > 30 ~ "3"))
df1$duration <- factor(df1$duration)
head(df1, 25)

```

```{r}
library(rpart)
library(rpart.plot)
tree <- rpart(duration ~ arrests+know_ini+region+coup,  df1, control = rpart.control(minsplit = 1, minbucket = 1, cp = 0.001))
x <- tail(df1)

result <- predict(tree, x)
print(result)
```


```{r}
rpart.plot(tree, box.palette = "white")
```


The decision tree model isn't working as well as I hoped, Let's try a random forest algorithm.

```{r}
library(randomForest)
library(caret)

rf_df <- subset(df1, select = -c(country, date, day_span, jul_start_date, jul_end_date, e_length))

trainIndex <- createDataPartition(rf_df$duration, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- rf_df[ trainIndex,]
test_data  <- rf_df[-trainIndex,]

rf <- randomForest(duration ~ ., data = train_data, na.action = na.omit)
rf
varImpPlot(rf)
```


Let's try a multinomial logistic regression model for comparison

```{r}
library(nnet)
library(caret)



MLR <- multinom(duration ~ ., train_data, na.action = na.omit)
#summary(MLR)
test_mlr <- predict(MLR, test_data)

confusionMatrix(test_mlr, test_data$duration)
```
```{r}
test_rf <- predict(rf, test_data)
confusionMatrix(test_rf, test_data$duration)
```

Looks like the random forrest model significantly out-performs the multi-nomial logistic regression model. It looks like most of the prediction errors are made in the class 2. The "longer than a week, less than a month" events are the hardest to identify. Thus let's try a binanry classification model to see if there is a notable improvement in accuracy.

```{r}
df_binary <- df1 %>% mutate(duration = case_when(day_span <= 15 ~ "0",
                            day_span > 15 ~ "1"))
df_binary$duration <- factor(df_binary$duration)
head(df_binary, 25)


```
```{r}
rf_df2 <- subset(df_binary, select = -c(country, date, day_span, jul_start_date, jul_end_date, e_length))

trainIndex <- createDataPartition(rf_df2$duration, p = .8, 
                                  list = FALSE, 
                                  times = 1)
binary_train_data <- rf_df2[ trainIndex,]
binary_test_data  <- rf_df2[-trainIndex,]

rf_binary <- randomForest(duration ~ ., data = binary_train_data, na.action = na.omit)
rf_binary
varImpPlot(rf_binary)
```

```{r}
test_rf_binary <- predict(rf_binary, binary_test_data)
confusionMatrix(test_rf_binary, binary_test_data$duration)
```
```{r}
rf_df3 <- subset(df_binary, select = -c(country, date, day, month, day_span, jul_start_date, jul_end_date, e_length))

trainIndex <- createDataPartition(rf_df3$duration, p = .8, 
                                  list = FALSE, 
                                  times = 1)
binary_train_data3 <- rf_df3[ trainIndex,]
binary_test_data3  <- rf_df3[-trainIndex,]

rf_binary3 <- randomForest(duration ~ ., data = binary_train_data3, na.action = na.omit)
rf_binary3
varImpPlot(rf_binary3)

```
```{r}
test_rf_binary3 <- predict(rf_binary3, binary_test_data3)
confusionMatrix(test_rf_binary3, binary_test_data3$duration)
```


```{r}
plot(df1$month)
```

```{r}
df_16_7 <- df_binary %>% mutate(day16 = factor(ifelse(day == 16, 1,0)),
                            month7 = factor(ifelse(month == 7, 1,0)))
head(df_16_7)
```

```{r}
rf_df4 <- subset(df_16_7, select = -c(country, date, day, month, day_span, jul_start_date, jul_end_date, e_length))

trainIndex <- createDataPartition(rf_df4$duration, p = .8, 
                                  list = FALSE, 
                                  times = 1)
binary_train_data4 <- rf_df4[ trainIndex,]
binary_test_data4  <- rf_df4[-trainIndex,]

rf_binary4 <- randomForest(duration ~ ., data = binary_train_data4, na.action = na.omit)
rf_binary4
varImpPlot(rf_binary4)
```

```{r}
test_rf_binary4 <- predict(rf_binary4, binary_test_data4)
confusionMatrix(test_rf_binary3, binary_test_data3$duration, positive = '1')
```

I'm going to replace the day variable with a categorical 'week' variable (week of the month)
```{r}
df_binary <- df_binary %>% mutate(week = case_when(day <= 7 ~ "1",
                            day > 7 & day <= 14 ~ "2", 
                           day > 14 & day <= 21 ~ "3",
                           day > 21 & day <= 28 ~ "4",
                           day > 28 ~ "5"))
df_binary$week <- factor(df_binary$week)
head(df_binary)
```

```{r}
rf_df5 <- subset(df_binary, select = -c(country, date, day, day_span, jul_start_date, jul_end_date, e_length))

trainIndex <- createDataPartition(rf_df5$duration, p = .8, 
                                  list = FALSE, 
                                  times = 1)
binary_train_data5 <- rf_df5[ trainIndex,]
binary_test_data5  <- rf_df5[-trainIndex,]

rf_binary5 <- randomForest(duration ~ ., data = binary_train_data5, na.action = na.omit)
rf_binary5
varImpPlot(rf_binary5)
```
```{r}
test_rf_binary5 <- predict(rf_binary5, binary_test_data5)
confusionMatrix(test_rf_binary5, binary_test_data5$duration, positive = '1')
```

That looks like a pretty decent model. Lets compare with a logistic regression once more

```{r}
logit1 <- glm(duration ~ ., family = 'binomial', data = binary_train_data5)
summary(logit1)

```

```{r}
test_logit1 <- predict(logit1, binary_test_data5)
predicted.classes <- ifelse(test_logit1 > 0.5, 1, 0)
confusionMatrix(factor(predicted.classes), binary_test_data5$duration, positive = '1')
```

```{r}
rf_t10 <- subset(df_binary, select = c(month, week, loc_type, region, weapon, n_killed_a, know_ini, ev_type, sc_animosity, anti_gov_sentmnts, duration))

trainIndex <- createDataPartition(rf_t10$duration, p = .8, 
                                  list = FALSE, 
                                  times = 1)
binary_train_t10 <- rf_t10[ trainIndex,]
binary_test_t10  <- rf_t10[-trainIndex,]

rf_t10 <- randomForest(duration ~ ., data = binary_train_t10, na.action = na.omit)
rf_t10
varImpPlot(rf_t10)
```
```{r}
test_rf_t10 <- predict(rf_t10, binary_test_t10)
confusionMatrix(test_rf_t10, binary_test_t10$duration, positive = '1')
```

This is a much more simple model that has similar perfomance to the one with all the variables. 






